{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.0,
  "eval_steps": 500,
  "global_step": 1066,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.018779342723004695,
      "grad_norm": 3.590290069580078,
      "learning_rate": 4.957786116322702e-05,
      "loss": 2.2562,
      "step": 10
    },
    {
      "epoch": 0.03755868544600939,
      "grad_norm": 2.7772912979125977,
      "learning_rate": 4.910881801125704e-05,
      "loss": 0.6671,
      "step": 20
    },
    {
      "epoch": 0.056338028169014086,
      "grad_norm": 3.3501040935516357,
      "learning_rate": 4.863977485928706e-05,
      "loss": 0.6079,
      "step": 30
    },
    {
      "epoch": 0.07511737089201878,
      "grad_norm": 3.3040242195129395,
      "learning_rate": 4.817073170731707e-05,
      "loss": 0.6085,
      "step": 40
    },
    {
      "epoch": 0.09389671361502347,
      "grad_norm": 3.0791409015655518,
      "learning_rate": 4.7701688555347094e-05,
      "loss": 0.564,
      "step": 50
    },
    {
      "epoch": 0.11267605633802817,
      "grad_norm": 2.862999439239502,
      "learning_rate": 4.7232645403377116e-05,
      "loss": 0.5318,
      "step": 60
    },
    {
      "epoch": 0.13145539906103287,
      "grad_norm": 2.587397813796997,
      "learning_rate": 4.676360225140713e-05,
      "loss": 0.5401,
      "step": 70
    },
    {
      "epoch": 0.15023474178403756,
      "grad_norm": 2.8100385665893555,
      "learning_rate": 4.629455909943715e-05,
      "loss": 0.5444,
      "step": 80
    },
    {
      "epoch": 0.16901408450704225,
      "grad_norm": 2.460928440093994,
      "learning_rate": 4.5825515947467166e-05,
      "loss": 0.5024,
      "step": 90
    },
    {
      "epoch": 0.18779342723004694,
      "grad_norm": 2.416576385498047,
      "learning_rate": 4.535647279549719e-05,
      "loss": 0.5029,
      "step": 100
    },
    {
      "epoch": 0.20657276995305165,
      "grad_norm": 2.8824846744537354,
      "learning_rate": 4.488742964352721e-05,
      "loss": 0.5586,
      "step": 110
    },
    {
      "epoch": 0.22535211267605634,
      "grad_norm": 2.3704030513763428,
      "learning_rate": 4.4418386491557224e-05,
      "loss": 0.5314,
      "step": 120
    },
    {
      "epoch": 0.24413145539906103,
      "grad_norm": 2.5991082191467285,
      "learning_rate": 4.3949343339587246e-05,
      "loss": 0.5164,
      "step": 130
    },
    {
      "epoch": 0.26291079812206575,
      "grad_norm": 2.133272171020508,
      "learning_rate": 4.348030018761726e-05,
      "loss": 0.4909,
      "step": 140
    },
    {
      "epoch": 0.28169014084507044,
      "grad_norm": 2.284968376159668,
      "learning_rate": 4.301125703564728e-05,
      "loss": 0.486,
      "step": 150
    },
    {
      "epoch": 0.3004694835680751,
      "grad_norm": 2.344393730163574,
      "learning_rate": 4.25422138836773e-05,
      "loss": 0.5369,
      "step": 160
    },
    {
      "epoch": 0.3192488262910798,
      "grad_norm": 2.1911752223968506,
      "learning_rate": 4.207317073170732e-05,
      "loss": 0.527,
      "step": 170
    },
    {
      "epoch": 0.3380281690140845,
      "grad_norm": 2.4019875526428223,
      "learning_rate": 4.160412757973734e-05,
      "loss": 0.5097,
      "step": 180
    },
    {
      "epoch": 0.3568075117370892,
      "grad_norm": 2.078458309173584,
      "learning_rate": 4.1135084427767354e-05,
      "loss": 0.4759,
      "step": 190
    },
    {
      "epoch": 0.3755868544600939,
      "grad_norm": 2.375509262084961,
      "learning_rate": 4.0666041275797375e-05,
      "loss": 0.4968,
      "step": 200
    },
    {
      "epoch": 0.39436619718309857,
      "grad_norm": 2.2471261024475098,
      "learning_rate": 4.01969981238274e-05,
      "loss": 0.5167,
      "step": 210
    },
    {
      "epoch": 0.4131455399061033,
      "grad_norm": 1.9798007011413574,
      "learning_rate": 3.972795497185741e-05,
      "loss": 0.5033,
      "step": 220
    },
    {
      "epoch": 0.431924882629108,
      "grad_norm": 1.750465989112854,
      "learning_rate": 3.925891181988743e-05,
      "loss": 0.5055,
      "step": 230
    },
    {
      "epoch": 0.4507042253521127,
      "grad_norm": 1.684137225151062,
      "learning_rate": 3.878986866791745e-05,
      "loss": 0.4992,
      "step": 240
    },
    {
      "epoch": 0.4694835680751174,
      "grad_norm": 1.695458173751831,
      "learning_rate": 3.832082551594747e-05,
      "loss": 0.4611,
      "step": 250
    },
    {
      "epoch": 0.48826291079812206,
      "grad_norm": 2.094841480255127,
      "learning_rate": 3.785178236397749e-05,
      "loss": 0.4862,
      "step": 260
    },
    {
      "epoch": 0.5070422535211268,
      "grad_norm": 1.674609661102295,
      "learning_rate": 3.7382739212007505e-05,
      "loss": 0.5151,
      "step": 270
    },
    {
      "epoch": 0.5258215962441315,
      "grad_norm": 1.903937578201294,
      "learning_rate": 3.691369606003752e-05,
      "loss": 0.4735,
      "step": 280
    },
    {
      "epoch": 0.5446009389671361,
      "grad_norm": 1.3624335527420044,
      "learning_rate": 3.644465290806754e-05,
      "loss": 0.4751,
      "step": 290
    },
    {
      "epoch": 0.5633802816901409,
      "grad_norm": 1.907494068145752,
      "learning_rate": 3.597560975609756e-05,
      "loss": 0.4601,
      "step": 300
    },
    {
      "epoch": 0.5821596244131455,
      "grad_norm": 1.5389124155044556,
      "learning_rate": 3.5506566604127585e-05,
      "loss": 0.4596,
      "step": 310
    },
    {
      "epoch": 0.6009389671361502,
      "grad_norm": 1.6502070426940918,
      "learning_rate": 3.50375234521576e-05,
      "loss": 0.4997,
      "step": 320
    },
    {
      "epoch": 0.6197183098591549,
      "grad_norm": 1.711584448814392,
      "learning_rate": 3.4568480300187614e-05,
      "loss": 0.5219,
      "step": 330
    },
    {
      "epoch": 0.6384976525821596,
      "grad_norm": 1.5291050672531128,
      "learning_rate": 3.409943714821764e-05,
      "loss": 0.474,
      "step": 340
    },
    {
      "epoch": 0.6572769953051644,
      "grad_norm": 1.703834891319275,
      "learning_rate": 3.363039399624766e-05,
      "loss": 0.4687,
      "step": 350
    },
    {
      "epoch": 0.676056338028169,
      "grad_norm": 1.5483787059783936,
      "learning_rate": 3.316135084427768e-05,
      "loss": 0.4976,
      "step": 360
    },
    {
      "epoch": 0.6948356807511737,
      "grad_norm": 1.5691503286361694,
      "learning_rate": 3.269230769230769e-05,
      "loss": 0.5044,
      "step": 370
    },
    {
      "epoch": 0.7136150234741784,
      "grad_norm": 1.7252740859985352,
      "learning_rate": 3.222326454033771e-05,
      "loss": 0.4907,
      "step": 380
    },
    {
      "epoch": 0.7323943661971831,
      "grad_norm": 1.529793381690979,
      "learning_rate": 3.1754221388367736e-05,
      "loss": 0.4707,
      "step": 390
    },
    {
      "epoch": 0.7511737089201878,
      "grad_norm": 1.8561818599700928,
      "learning_rate": 3.128517823639775e-05,
      "loss": 0.4773,
      "step": 400
    },
    {
      "epoch": 0.7699530516431925,
      "grad_norm": 1.5347793102264404,
      "learning_rate": 3.0816135084427765e-05,
      "loss": 0.4582,
      "step": 410
    },
    {
      "epoch": 0.7887323943661971,
      "grad_norm": 1.4525957107543945,
      "learning_rate": 3.0347091932457787e-05,
      "loss": 0.522,
      "step": 420
    },
    {
      "epoch": 0.8075117370892019,
      "grad_norm": 1.5618313550949097,
      "learning_rate": 2.9878048780487805e-05,
      "loss": 0.4593,
      "step": 430
    },
    {
      "epoch": 0.8262910798122066,
      "grad_norm": 1.5102434158325195,
      "learning_rate": 2.9409005628517826e-05,
      "loss": 0.5113,
      "step": 440
    },
    {
      "epoch": 0.8450704225352113,
      "grad_norm": 1.561517357826233,
      "learning_rate": 2.8939962476547845e-05,
      "loss": 0.4881,
      "step": 450
    },
    {
      "epoch": 0.863849765258216,
      "grad_norm": 1.454322099685669,
      "learning_rate": 2.847091932457786e-05,
      "loss": 0.4707,
      "step": 460
    },
    {
      "epoch": 0.8826291079812206,
      "grad_norm": 1.2985053062438965,
      "learning_rate": 2.8001876172607884e-05,
      "loss": 0.473,
      "step": 470
    },
    {
      "epoch": 0.9014084507042254,
      "grad_norm": 1.163148045539856,
      "learning_rate": 2.75328330206379e-05,
      "loss": 0.4809,
      "step": 480
    },
    {
      "epoch": 0.92018779342723,
      "grad_norm": 1.3521466255187988,
      "learning_rate": 2.706378986866792e-05,
      "loss": 0.4985,
      "step": 490
    },
    {
      "epoch": 0.9389671361502347,
      "grad_norm": 1.2716171741485596,
      "learning_rate": 2.659474671669794e-05,
      "loss": 0.489,
      "step": 500
    },
    {
      "epoch": 0.9577464788732394,
      "grad_norm": 1.2270269393920898,
      "learning_rate": 2.6125703564727953e-05,
      "loss": 0.4485,
      "step": 510
    },
    {
      "epoch": 0.9765258215962441,
      "grad_norm": 1.5207254886627197,
      "learning_rate": 2.5656660412757978e-05,
      "loss": 0.4601,
      "step": 520
    },
    {
      "epoch": 0.9953051643192489,
      "grad_norm": 1.377993106842041,
      "learning_rate": 2.5187617260787993e-05,
      "loss": 0.4562,
      "step": 530
    },
    {
      "epoch": 1.0131455399061033,
      "grad_norm": 1.289974570274353,
      "learning_rate": 2.4718574108818014e-05,
      "loss": 0.4304,
      "step": 540
    },
    {
      "epoch": 1.031924882629108,
      "grad_norm": 1.708842158317566,
      "learning_rate": 2.4249530956848032e-05,
      "loss": 0.4161,
      "step": 550
    },
    {
      "epoch": 1.0507042253521126,
      "grad_norm": 1.3566527366638184,
      "learning_rate": 2.378048780487805e-05,
      "loss": 0.4283,
      "step": 560
    },
    {
      "epoch": 1.0694835680751174,
      "grad_norm": 1.3035136461257935,
      "learning_rate": 2.331144465290807e-05,
      "loss": 0.4183,
      "step": 570
    },
    {
      "epoch": 1.088262910798122,
      "grad_norm": 1.5994597673416138,
      "learning_rate": 2.2842401500938086e-05,
      "loss": 0.4298,
      "step": 580
    },
    {
      "epoch": 1.1070422535211268,
      "grad_norm": 1.5807212591171265,
      "learning_rate": 2.2373358348968108e-05,
      "loss": 0.4097,
      "step": 590
    },
    {
      "epoch": 1.1258215962441314,
      "grad_norm": 1.5358452796936035,
      "learning_rate": 2.1904315196998123e-05,
      "loss": 0.4008,
      "step": 600
    },
    {
      "epoch": 1.144600938967136,
      "grad_norm": 1.0829296112060547,
      "learning_rate": 2.1435272045028144e-05,
      "loss": 0.4183,
      "step": 610
    },
    {
      "epoch": 1.1633802816901408,
      "grad_norm": 1.4882923364639282,
      "learning_rate": 2.0966228893058162e-05,
      "loss": 0.4149,
      "step": 620
    },
    {
      "epoch": 1.1821596244131456,
      "grad_norm": 1.5853698253631592,
      "learning_rate": 2.049718574108818e-05,
      "loss": 0.4352,
      "step": 630
    },
    {
      "epoch": 1.2009389671361501,
      "grad_norm": 1.4636576175689697,
      "learning_rate": 2.0028142589118202e-05,
      "loss": 0.4229,
      "step": 640
    },
    {
      "epoch": 1.2197183098591549,
      "grad_norm": 1.5289067029953003,
      "learning_rate": 1.9559099437148216e-05,
      "loss": 0.3942,
      "step": 650
    },
    {
      "epoch": 1.2384976525821596,
      "grad_norm": 1.4317048788070679,
      "learning_rate": 1.9090056285178238e-05,
      "loss": 0.4203,
      "step": 660
    },
    {
      "epoch": 1.2572769953051643,
      "grad_norm": 1.7078635692596436,
      "learning_rate": 1.8621013133208256e-05,
      "loss": 0.4144,
      "step": 670
    },
    {
      "epoch": 1.276056338028169,
      "grad_norm": 1.6594678163528442,
      "learning_rate": 1.8151969981238277e-05,
      "loss": 0.4133,
      "step": 680
    },
    {
      "epoch": 1.2948356807511736,
      "grad_norm": 1.3778750896453857,
      "learning_rate": 1.7682926829268292e-05,
      "loss": 0.4091,
      "step": 690
    },
    {
      "epoch": 1.3136150234741784,
      "grad_norm": 1.541371464729309,
      "learning_rate": 1.7213883677298314e-05,
      "loss": 0.4683,
      "step": 700
    },
    {
      "epoch": 1.332394366197183,
      "grad_norm": 1.3222564458847046,
      "learning_rate": 1.674484052532833e-05,
      "loss": 0.4178,
      "step": 710
    },
    {
      "epoch": 1.3511737089201878,
      "grad_norm": 1.2962650060653687,
      "learning_rate": 1.627579737335835e-05,
      "loss": 0.388,
      "step": 720
    },
    {
      "epoch": 1.3699530516431926,
      "grad_norm": 1.307733416557312,
      "learning_rate": 1.5806754221388368e-05,
      "loss": 0.4029,
      "step": 730
    },
    {
      "epoch": 1.388732394366197,
      "grad_norm": 1.3561627864837646,
      "learning_rate": 1.5337711069418386e-05,
      "loss": 0.4132,
      "step": 740
    },
    {
      "epoch": 1.4075117370892019,
      "grad_norm": 1.3851498365402222,
      "learning_rate": 1.4868667917448406e-05,
      "loss": 0.4067,
      "step": 750
    },
    {
      "epoch": 1.4262910798122066,
      "grad_norm": 1.4694392681121826,
      "learning_rate": 1.4399624765478425e-05,
      "loss": 0.4509,
      "step": 760
    },
    {
      "epoch": 1.4450704225352113,
      "grad_norm": 1.6622320413589478,
      "learning_rate": 1.3930581613508442e-05,
      "loss": 0.4262,
      "step": 770
    },
    {
      "epoch": 1.463849765258216,
      "grad_norm": 1.2402385473251343,
      "learning_rate": 1.3461538461538462e-05,
      "loss": 0.3984,
      "step": 780
    },
    {
      "epoch": 1.4826291079812206,
      "grad_norm": 1.5558141469955444,
      "learning_rate": 1.2992495309568481e-05,
      "loss": 0.4189,
      "step": 790
    },
    {
      "epoch": 1.5014084507042254,
      "grad_norm": 1.3373202085494995,
      "learning_rate": 1.25234521575985e-05,
      "loss": 0.4217,
      "step": 800
    },
    {
      "epoch": 1.52018779342723,
      "grad_norm": 1.5485461950302124,
      "learning_rate": 1.2054409005628518e-05,
      "loss": 0.3885,
      "step": 810
    },
    {
      "epoch": 1.5389671361502346,
      "grad_norm": 1.440982699394226,
      "learning_rate": 1.1585365853658537e-05,
      "loss": 0.4388,
      "step": 820
    },
    {
      "epoch": 1.5577464788732396,
      "grad_norm": 1.5153768062591553,
      "learning_rate": 1.1116322701688555e-05,
      "loss": 0.3925,
      "step": 830
    },
    {
      "epoch": 1.576525821596244,
      "grad_norm": 1.5695303678512573,
      "learning_rate": 1.0647279549718575e-05,
      "loss": 0.4021,
      "step": 840
    },
    {
      "epoch": 1.5953051643192488,
      "grad_norm": 1.376006841659546,
      "learning_rate": 1.0178236397748593e-05,
      "loss": 0.4171,
      "step": 850
    },
    {
      "epoch": 1.6140845070422536,
      "grad_norm": 1.887694239616394,
      "learning_rate": 9.709193245778613e-06,
      "loss": 0.4403,
      "step": 860
    },
    {
      "epoch": 1.6328638497652581,
      "grad_norm": 1.7046607732772827,
      "learning_rate": 9.240150093808631e-06,
      "loss": 0.4164,
      "step": 870
    },
    {
      "epoch": 1.6516431924882629,
      "grad_norm": 1.436867356300354,
      "learning_rate": 8.77110694183865e-06,
      "loss": 0.4004,
      "step": 880
    },
    {
      "epoch": 1.6704225352112676,
      "grad_norm": 1.6336554288864136,
      "learning_rate": 8.302063789868667e-06,
      "loss": 0.4052,
      "step": 890
    },
    {
      "epoch": 1.6892018779342723,
      "grad_norm": 1.5713918209075928,
      "learning_rate": 7.833020637898687e-06,
      "loss": 0.412,
      "step": 900
    },
    {
      "epoch": 1.707981220657277,
      "grad_norm": 1.2850415706634521,
      "learning_rate": 7.363977485928705e-06,
      "loss": 0.3883,
      "step": 910
    },
    {
      "epoch": 1.7267605633802816,
      "grad_norm": 1.6479318141937256,
      "learning_rate": 6.894934333958725e-06,
      "loss": 0.42,
      "step": 920
    },
    {
      "epoch": 1.7455399061032864,
      "grad_norm": 1.4687576293945312,
      "learning_rate": 6.425891181988743e-06,
      "loss": 0.4074,
      "step": 930
    },
    {
      "epoch": 1.764319248826291,
      "grad_norm": 1.38376784324646,
      "learning_rate": 5.956848030018762e-06,
      "loss": 0.41,
      "step": 940
    },
    {
      "epoch": 1.7830985915492956,
      "grad_norm": 1.4805713891983032,
      "learning_rate": 5.487804878048781e-06,
      "loss": 0.4117,
      "step": 950
    },
    {
      "epoch": 1.8018779342723006,
      "grad_norm": 1.5533416271209717,
      "learning_rate": 5.018761726078799e-06,
      "loss": 0.446,
      "step": 960
    },
    {
      "epoch": 1.820657276995305,
      "grad_norm": 1.4135315418243408,
      "learning_rate": 4.549718574108818e-06,
      "loss": 0.3748,
      "step": 970
    },
    {
      "epoch": 1.8394366197183099,
      "grad_norm": 1.5588880777359009,
      "learning_rate": 4.080675422138837e-06,
      "loss": 0.4245,
      "step": 980
    },
    {
      "epoch": 1.8582159624413146,
      "grad_norm": 1.6924049854278564,
      "learning_rate": 3.6116322701688554e-06,
      "loss": 0.4015,
      "step": 990
    },
    {
      "epoch": 1.8769953051643191,
      "grad_norm": 1.4009748697280884,
      "learning_rate": 3.1425891181988743e-06,
      "loss": 0.4187,
      "step": 1000
    },
    {
      "epoch": 1.895774647887324,
      "grad_norm": 1.6108989715576172,
      "learning_rate": 2.6735459662288933e-06,
      "loss": 0.4362,
      "step": 1010
    },
    {
      "epoch": 1.9145539906103286,
      "grad_norm": 1.4165345430374146,
      "learning_rate": 2.2045028142589118e-06,
      "loss": 0.4182,
      "step": 1020
    },
    {
      "epoch": 1.9333333333333333,
      "grad_norm": 1.4467699527740479,
      "learning_rate": 1.7354596622889307e-06,
      "loss": 0.4009,
      "step": 1030
    },
    {
      "epoch": 1.952112676056338,
      "grad_norm": 1.7793515920639038,
      "learning_rate": 1.2664165103189494e-06,
      "loss": 0.4243,
      "step": 1040
    },
    {
      "epoch": 1.9708920187793426,
      "grad_norm": 1.3375658988952637,
      "learning_rate": 7.97373358348968e-07,
      "loss": 0.3784,
      "step": 1050
    },
    {
      "epoch": 1.9896713615023476,
      "grad_norm": 1.5092992782592773,
      "learning_rate": 3.2833020637898693e-07,
      "loss": 0.4021,
      "step": 1060
    }
  ],
  "logging_steps": 10,
  "max_steps": 1066,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 2,
  "save_steps": 100,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 556552028160000.0,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
