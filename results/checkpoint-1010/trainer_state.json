{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.0,
  "eval_steps": 500,
  "global_step": 1010,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.019801980198019802,
      "grad_norm": 17.53215217590332,
      "learning_rate": 4.955445544554456e-05,
      "loss": 3.8033,
      "step": 10
    },
    {
      "epoch": 0.039603960396039604,
      "grad_norm": 16.237783432006836,
      "learning_rate": 4.905940594059406e-05,
      "loss": 1.78,
      "step": 20
    },
    {
      "epoch": 0.0594059405940594,
      "grad_norm": 19.226680755615234,
      "learning_rate": 4.8564356435643565e-05,
      "loss": 1.8073,
      "step": 30
    },
    {
      "epoch": 0.07920792079207921,
      "grad_norm": 11.550127029418945,
      "learning_rate": 4.806930693069307e-05,
      "loss": 1.352,
      "step": 40
    },
    {
      "epoch": 0.09900990099009901,
      "grad_norm": 12.711520195007324,
      "learning_rate": 4.7574257425742576e-05,
      "loss": 1.2743,
      "step": 50
    },
    {
      "epoch": 0.1188118811881188,
      "grad_norm": 18.765104293823242,
      "learning_rate": 4.7079207920792075e-05,
      "loss": 1.3589,
      "step": 60
    },
    {
      "epoch": 0.13861386138613863,
      "grad_norm": 26.460155487060547,
      "learning_rate": 4.658415841584159e-05,
      "loss": 1.3503,
      "step": 70
    },
    {
      "epoch": 0.15841584158415842,
      "grad_norm": 7.500312328338623,
      "learning_rate": 4.6089108910891094e-05,
      "loss": 1.5961,
      "step": 80
    },
    {
      "epoch": 0.1782178217821782,
      "grad_norm": 9.92701244354248,
      "learning_rate": 4.55940594059406e-05,
      "loss": 1.462,
      "step": 90
    },
    {
      "epoch": 0.19801980198019803,
      "grad_norm": 16.94554328918457,
      "learning_rate": 4.5099009900990106e-05,
      "loss": 1.8024,
      "step": 100
    },
    {
      "epoch": 0.21782178217821782,
      "grad_norm": 10.092145919799805,
      "learning_rate": 4.4603960396039605e-05,
      "loss": 1.1959,
      "step": 110
    },
    {
      "epoch": 0.2376237623762376,
      "grad_norm": 14.92078971862793,
      "learning_rate": 4.410891089108911e-05,
      "loss": 1.456,
      "step": 120
    },
    {
      "epoch": 0.25742574257425743,
      "grad_norm": 10.821744918823242,
      "learning_rate": 4.3613861386138617e-05,
      "loss": 1.7066,
      "step": 130
    },
    {
      "epoch": 0.27722772277227725,
      "grad_norm": 8.860390663146973,
      "learning_rate": 4.311881188118812e-05,
      "loss": 1.595,
      "step": 140
    },
    {
      "epoch": 0.297029702970297,
      "grad_norm": 12.037618637084961,
      "learning_rate": 4.262376237623762e-05,
      "loss": 1.2334,
      "step": 150
    },
    {
      "epoch": 0.31683168316831684,
      "grad_norm": 7.183943271636963,
      "learning_rate": 4.212871287128713e-05,
      "loss": 1.1517,
      "step": 160
    },
    {
      "epoch": 0.33663366336633666,
      "grad_norm": 10.35153865814209,
      "learning_rate": 4.163366336633663e-05,
      "loss": 1.0519,
      "step": 170
    },
    {
      "epoch": 0.3564356435643564,
      "grad_norm": 9.019620895385742,
      "learning_rate": 4.113861386138614e-05,
      "loss": 1.2593,
      "step": 180
    },
    {
      "epoch": 0.37623762376237624,
      "grad_norm": 7.180078983306885,
      "learning_rate": 4.0643564356435645e-05,
      "loss": 1.2396,
      "step": 190
    },
    {
      "epoch": 0.39603960396039606,
      "grad_norm": 8.249787330627441,
      "learning_rate": 4.014851485148515e-05,
      "loss": 1.6174,
      "step": 200
    },
    {
      "epoch": 0.4158415841584158,
      "grad_norm": 8.425040245056152,
      "learning_rate": 3.965346534653466e-05,
      "loss": 1.2888,
      "step": 210
    },
    {
      "epoch": 0.43564356435643564,
      "grad_norm": 8.647844314575195,
      "learning_rate": 3.915841584158416e-05,
      "loss": 1.2784,
      "step": 220
    },
    {
      "epoch": 0.45544554455445546,
      "grad_norm": 12.849869728088379,
      "learning_rate": 3.866336633663367e-05,
      "loss": 1.3786,
      "step": 230
    },
    {
      "epoch": 0.4752475247524752,
      "grad_norm": 7.833290100097656,
      "learning_rate": 3.816831683168317e-05,
      "loss": 1.0539,
      "step": 240
    },
    {
      "epoch": 0.49504950495049505,
      "grad_norm": 27.223100662231445,
      "learning_rate": 3.7673267326732673e-05,
      "loss": 1.4111,
      "step": 250
    },
    {
      "epoch": 0.5148514851485149,
      "grad_norm": 12.708513259887695,
      "learning_rate": 3.717821782178218e-05,
      "loss": 1.1697,
      "step": 260
    },
    {
      "epoch": 0.5346534653465347,
      "grad_norm": 8.207667350769043,
      "learning_rate": 3.6683168316831685e-05,
      "loss": 1.0637,
      "step": 270
    },
    {
      "epoch": 0.5544554455445545,
      "grad_norm": 8.342345237731934,
      "learning_rate": 3.618811881188119e-05,
      "loss": 1.3173,
      "step": 280
    },
    {
      "epoch": 0.5742574257425742,
      "grad_norm": 5.849869251251221,
      "learning_rate": 3.569306930693069e-05,
      "loss": 0.9013,
      "step": 290
    },
    {
      "epoch": 0.594059405940594,
      "grad_norm": 11.060399055480957,
      "learning_rate": 3.5198019801980196e-05,
      "loss": 1.6469,
      "step": 300
    },
    {
      "epoch": 0.6138613861386139,
      "grad_norm": 6.562955856323242,
      "learning_rate": 3.47029702970297e-05,
      "loss": 1.57,
      "step": 310
    },
    {
      "epoch": 0.6336633663366337,
      "grad_norm": 9.700984001159668,
      "learning_rate": 3.4207920792079214e-05,
      "loss": 1.7325,
      "step": 320
    },
    {
      "epoch": 0.6534653465346535,
      "grad_norm": 8.156394958496094,
      "learning_rate": 3.3712871287128714e-05,
      "loss": 1.0547,
      "step": 330
    },
    {
      "epoch": 0.6732673267326733,
      "grad_norm": 5.096841335296631,
      "learning_rate": 3.321782178217822e-05,
      "loss": 1.1834,
      "step": 340
    },
    {
      "epoch": 0.693069306930693,
      "grad_norm": 10.918048858642578,
      "learning_rate": 3.2722772277227725e-05,
      "loss": 1.2489,
      "step": 350
    },
    {
      "epoch": 0.7128712871287128,
      "grad_norm": 9.968969345092773,
      "learning_rate": 3.222772277227723e-05,
      "loss": 1.5046,
      "step": 360
    },
    {
      "epoch": 0.7326732673267327,
      "grad_norm": 11.726243019104004,
      "learning_rate": 3.173267326732674e-05,
      "loss": 1.2109,
      "step": 370
    },
    {
      "epoch": 0.7524752475247525,
      "grad_norm": 7.367700576782227,
      "learning_rate": 3.1237623762376236e-05,
      "loss": 1.2395,
      "step": 380
    },
    {
      "epoch": 0.7722772277227723,
      "grad_norm": 12.363116264343262,
      "learning_rate": 3.074257425742574e-05,
      "loss": 1.5258,
      "step": 390
    },
    {
      "epoch": 0.7920792079207921,
      "grad_norm": 8.921966552734375,
      "learning_rate": 3.0247524752475248e-05,
      "loss": 1.1865,
      "step": 400
    },
    {
      "epoch": 0.8118811881188119,
      "grad_norm": 5.968494892120361,
      "learning_rate": 2.9752475247524754e-05,
      "loss": 1.353,
      "step": 410
    },
    {
      "epoch": 0.8316831683168316,
      "grad_norm": 9.231634140014648,
      "learning_rate": 2.9257425742574256e-05,
      "loss": 1.5901,
      "step": 420
    },
    {
      "epoch": 0.8514851485148515,
      "grad_norm": 5.254511833190918,
      "learning_rate": 2.8762376237623762e-05,
      "loss": 1.0001,
      "step": 430
    },
    {
      "epoch": 0.8712871287128713,
      "grad_norm": 5.73327112197876,
      "learning_rate": 2.8267326732673265e-05,
      "loss": 1.2398,
      "step": 440
    },
    {
      "epoch": 0.8910891089108911,
      "grad_norm": 5.476203918457031,
      "learning_rate": 2.7772277227722777e-05,
      "loss": 1.261,
      "step": 450
    },
    {
      "epoch": 0.9108910891089109,
      "grad_norm": 8.471155166625977,
      "learning_rate": 2.727722772277228e-05,
      "loss": 1.3498,
      "step": 460
    },
    {
      "epoch": 0.9306930693069307,
      "grad_norm": 6.138123989105225,
      "learning_rate": 2.6782178217821786e-05,
      "loss": 1.4525,
      "step": 470
    },
    {
      "epoch": 0.9504950495049505,
      "grad_norm": 8.565797805786133,
      "learning_rate": 2.6287128712871288e-05,
      "loss": 1.2236,
      "step": 480
    },
    {
      "epoch": 0.9702970297029703,
      "grad_norm": 8.017522811889648,
      "learning_rate": 2.5792079207920794e-05,
      "loss": 1.6205,
      "step": 490
    },
    {
      "epoch": 0.9900990099009901,
      "grad_norm": 5.689511299133301,
      "learning_rate": 2.52970297029703e-05,
      "loss": 1.2263,
      "step": 500
    },
    {
      "epoch": 1.00990099009901,
      "grad_norm": 7.720137596130371,
      "learning_rate": 2.4801980198019802e-05,
      "loss": 1.7355,
      "step": 510
    },
    {
      "epoch": 1.0297029702970297,
      "grad_norm": 7.72098970413208,
      "learning_rate": 2.4306930693069308e-05,
      "loss": 1.3568,
      "step": 520
    },
    {
      "epoch": 1.0495049504950495,
      "grad_norm": 5.268364429473877,
      "learning_rate": 2.381188118811881e-05,
      "loss": 0.94,
      "step": 530
    },
    {
      "epoch": 1.0693069306930694,
      "grad_norm": 8.703950881958008,
      "learning_rate": 2.331683168316832e-05,
      "loss": 0.9785,
      "step": 540
    },
    {
      "epoch": 1.0891089108910892,
      "grad_norm": 9.26634407043457,
      "learning_rate": 2.2821782178217822e-05,
      "loss": 1.439,
      "step": 550
    },
    {
      "epoch": 1.108910891089109,
      "grad_norm": 4.3488311767578125,
      "learning_rate": 2.2326732673267328e-05,
      "loss": 0.7697,
      "step": 560
    },
    {
      "epoch": 1.1287128712871288,
      "grad_norm": 10.7388916015625,
      "learning_rate": 2.1831683168316834e-05,
      "loss": 1.1557,
      "step": 570
    },
    {
      "epoch": 1.1485148514851484,
      "grad_norm": 5.503641128540039,
      "learning_rate": 2.1336633663366336e-05,
      "loss": 1.2958,
      "step": 580
    },
    {
      "epoch": 1.1683168316831682,
      "grad_norm": 6.240235328674316,
      "learning_rate": 2.0841584158415842e-05,
      "loss": 0.8771,
      "step": 590
    },
    {
      "epoch": 1.188118811881188,
      "grad_norm": 4.979982376098633,
      "learning_rate": 2.0346534653465345e-05,
      "loss": 1.0644,
      "step": 600
    },
    {
      "epoch": 1.2079207920792079,
      "grad_norm": 8.218889236450195,
      "learning_rate": 1.9851485148514854e-05,
      "loss": 1.009,
      "step": 610
    },
    {
      "epoch": 1.2277227722772277,
      "grad_norm": 8.41205883026123,
      "learning_rate": 1.9356435643564357e-05,
      "loss": 0.8518,
      "step": 620
    },
    {
      "epoch": 1.2475247524752475,
      "grad_norm": 7.6995978355407715,
      "learning_rate": 1.8861386138613862e-05,
      "loss": 1.4615,
      "step": 630
    },
    {
      "epoch": 1.2673267326732673,
      "grad_norm": 12.657734870910645,
      "learning_rate": 1.8366336633663368e-05,
      "loss": 1.1258,
      "step": 640
    },
    {
      "epoch": 1.2871287128712872,
      "grad_norm": 6.115076065063477,
      "learning_rate": 1.787128712871287e-05,
      "loss": 0.7991,
      "step": 650
    },
    {
      "epoch": 1.306930693069307,
      "grad_norm": 9.045825004577637,
      "learning_rate": 1.7376237623762377e-05,
      "loss": 1.1695,
      "step": 660
    },
    {
      "epoch": 1.3267326732673268,
      "grad_norm": 6.738943576812744,
      "learning_rate": 1.6881188118811882e-05,
      "loss": 1.0941,
      "step": 670
    },
    {
      "epoch": 1.3465346534653464,
      "grad_norm": 10.208261489868164,
      "learning_rate": 1.638613861386139e-05,
      "loss": 1.1944,
      "step": 680
    },
    {
      "epoch": 1.3663366336633662,
      "grad_norm": 8.563009262084961,
      "learning_rate": 1.589108910891089e-05,
      "loss": 0.8485,
      "step": 690
    },
    {
      "epoch": 1.386138613861386,
      "grad_norm": 6.76306676864624,
      "learning_rate": 1.5396039603960397e-05,
      "loss": 0.9898,
      "step": 700
    },
    {
      "epoch": 1.4059405940594059,
      "grad_norm": 4.527449607849121,
      "learning_rate": 1.4900990099009901e-05,
      "loss": 0.9238,
      "step": 710
    },
    {
      "epoch": 1.4257425742574257,
      "grad_norm": 9.081498146057129,
      "learning_rate": 1.4405940594059405e-05,
      "loss": 1.1624,
      "step": 720
    },
    {
      "epoch": 1.4455445544554455,
      "grad_norm": 8.502140998840332,
      "learning_rate": 1.3910891089108913e-05,
      "loss": 1.1455,
      "step": 730
    },
    {
      "epoch": 1.4653465346534653,
      "grad_norm": 5.4738450050354,
      "learning_rate": 1.3415841584158417e-05,
      "loss": 1.0505,
      "step": 740
    },
    {
      "epoch": 1.4851485148514851,
      "grad_norm": 6.133364677429199,
      "learning_rate": 1.2920792079207921e-05,
      "loss": 1.1159,
      "step": 750
    },
    {
      "epoch": 1.504950495049505,
      "grad_norm": 5.242097854614258,
      "learning_rate": 1.2425742574257427e-05,
      "loss": 0.8158,
      "step": 760
    },
    {
      "epoch": 1.5247524752475248,
      "grad_norm": 4.877456188201904,
      "learning_rate": 1.1930693069306931e-05,
      "loss": 0.7879,
      "step": 770
    },
    {
      "epoch": 1.5445544554455446,
      "grad_norm": 6.658653259277344,
      "learning_rate": 1.1435643564356437e-05,
      "loss": 1.1624,
      "step": 780
    },
    {
      "epoch": 1.5643564356435644,
      "grad_norm": 7.197543144226074,
      "learning_rate": 1.0940594059405941e-05,
      "loss": 0.873,
      "step": 790
    },
    {
      "epoch": 1.5841584158415842,
      "grad_norm": 4.569514274597168,
      "learning_rate": 1.0445544554455445e-05,
      "loss": 0.9047,
      "step": 800
    },
    {
      "epoch": 1.603960396039604,
      "grad_norm": 9.12827205657959,
      "learning_rate": 9.950495049504951e-06,
      "loss": 1.1438,
      "step": 810
    },
    {
      "epoch": 1.6237623762376239,
      "grad_norm": 6.897163391113281,
      "learning_rate": 9.455445544554455e-06,
      "loss": 1.1281,
      "step": 820
    },
    {
      "epoch": 1.6435643564356437,
      "grad_norm": 9.451577186584473,
      "learning_rate": 8.960396039603961e-06,
      "loss": 0.9676,
      "step": 830
    },
    {
      "epoch": 1.6633663366336635,
      "grad_norm": 4.714742660522461,
      "learning_rate": 8.465346534653467e-06,
      "loss": 0.8443,
      "step": 840
    },
    {
      "epoch": 1.6831683168316833,
      "grad_norm": 5.719674587249756,
      "learning_rate": 7.970297029702971e-06,
      "loss": 1.324,
      "step": 850
    },
    {
      "epoch": 1.702970297029703,
      "grad_norm": 8.834479331970215,
      "learning_rate": 7.475247524752475e-06,
      "loss": 0.9404,
      "step": 860
    },
    {
      "epoch": 1.7227722772277227,
      "grad_norm": 9.534585952758789,
      "learning_rate": 6.980198019801981e-06,
      "loss": 1.0821,
      "step": 870
    },
    {
      "epoch": 1.7425742574257426,
      "grad_norm": 9.940183639526367,
      "learning_rate": 6.485148514851485e-06,
      "loss": 1.0758,
      "step": 880
    },
    {
      "epoch": 1.7623762376237624,
      "grad_norm": 11.728017807006836,
      "learning_rate": 5.99009900990099e-06,
      "loss": 1.1115,
      "step": 890
    },
    {
      "epoch": 1.7821782178217822,
      "grad_norm": 6.823618412017822,
      "learning_rate": 5.495049504950495e-06,
      "loss": 0.9866,
      "step": 900
    },
    {
      "epoch": 1.801980198019802,
      "grad_norm": 11.292282104492188,
      "learning_rate": 5e-06,
      "loss": 0.9159,
      "step": 910
    },
    {
      "epoch": 1.8217821782178216,
      "grad_norm": 6.746927261352539,
      "learning_rate": 4.504950495049505e-06,
      "loss": 0.805,
      "step": 920
    },
    {
      "epoch": 1.8415841584158414,
      "grad_norm": 9.626761436462402,
      "learning_rate": 4.0099009900990104e-06,
      "loss": 0.987,
      "step": 930
    },
    {
      "epoch": 1.8613861386138613,
      "grad_norm": 7.1907958984375,
      "learning_rate": 3.5148514851485155e-06,
      "loss": 0.9294,
      "step": 940
    },
    {
      "epoch": 1.881188118811881,
      "grad_norm": 7.482592582702637,
      "learning_rate": 3.01980198019802e-06,
      "loss": 0.9318,
      "step": 950
    },
    {
      "epoch": 1.900990099009901,
      "grad_norm": 4.826028347015381,
      "learning_rate": 2.524752475247525e-06,
      "loss": 0.8134,
      "step": 960
    },
    {
      "epoch": 1.9207920792079207,
      "grad_norm": 10.233834266662598,
      "learning_rate": 2.0297029702970297e-06,
      "loss": 1.2856,
      "step": 970
    },
    {
      "epoch": 1.9405940594059405,
      "grad_norm": 9.949292182922363,
      "learning_rate": 1.5346534653465347e-06,
      "loss": 0.6729,
      "step": 980
    },
    {
      "epoch": 1.9603960396039604,
      "grad_norm": 11.940942764282227,
      "learning_rate": 1.0396039603960397e-06,
      "loss": 0.9356,
      "step": 990
    },
    {
      "epoch": 1.9801980198019802,
      "grad_norm": 7.101775169372559,
      "learning_rate": 5.445544554455445e-07,
      "loss": 1.1746,
      "step": 1000
    },
    {
      "epoch": 2.0,
      "grad_norm": 3.8189947605133057,
      "learning_rate": 4.950495049504951e-08,
      "loss": 0.9914,
      "step": 1010
    }
  ],
  "logging_steps": 10,
  "max_steps": 1010,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 2,
  "save_steps": 100,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 51543936000000.0,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
